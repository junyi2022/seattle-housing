---
title: "Predict Housing Value"
author: "Ziyi GUO, Junyi Yang"
output: html_document
---


```{r setup, include=FALSE}

# You can set some global options for knitting chunks

knitr::opts_chunk$set(echo = TRUE)

# Load some libraries

library(tidycensus)
library(tidyverse)
library(sf)
library(spdep)
library(caret)
library(ckanr)
library(FNN)
library(grid)
library(gridExtra)
library(ggcorrplot) # plot correlation plot
library(corrr)      # another way to plot correlation plot
library(kableExtra)
library(jtools)     # for regression model plots
library(ggstance) # to support jtools plots
library(ggpubr)    # plotting R^2 value on ggplot point scatter
library(broom.mixed) # needed for effects plots
library(RColorBrewer)
library(dplyr)
library(ggplot2)
library(RColorBrewer)
library(spatstat)

# functions and data directory
root.dir = "https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/"

source("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r")

```


## Data Wrangling

```{r get data}

# initial housing data
housingData <- 
  read.csv(file.path("./dataWithoutcrime/kc_house_data.csv"))

Seattle.sf <- 
  housingData %>% 
  st_as_sf(coords = c("long", "lat"), crs = 4326, agr = "constant") %>%
  st_transform(crs = 2926) %>%
  mutate(price_per_sqft = price / sqft_lot)

# Seattle geo_Tracts
tracts <- 
  st_read("./dataWithoutcrime/2010_Census_Tract_Seattle.geojson") %>%
  st_transform(crs = 2926)

#Cut the housing data to fit into the Seattle boundary
Seattle.sf_tracts <- st_intersection(Seattle.sf, tracts)

Seattle.sf_tracts2 <- Seattle.sf_tracts %>%
  select(id, date, price ,bedrooms, bathrooms, sqft_living, sqft_lot, floors, waterfront, view, condition, grade, yr_built, yr_renovated, zipcode, price_per_sqft, geometry)


# Add-in Data

# Seattle neighborhood
neighborhood <- 
  st_read("./dataWithoutcrime/Neighborhood_Map.geojson") %>%
  st_transform(crs = 2926)

## social
crime <- read.csv(file.path("C:/Users/25077/Desktop/MUSA 508_PPA/5_/github/local/seattle-housing/dataWithoutcrime/SPD_Crime_Data.csv"))

Crime.sf <- 
  crime %>% 
  st_as_sf(coords = c("Longitude", "Latitude"), crs = 4326, agr = "constant") %>%
  st_transform(crs = 2926)


## Amenities
School <- 
  st_read("./dataWithoutcrime/Private_School.geojson") %>%
  st_transform(crs = 2926)

Parks <- 
  st_read("./dataWithoutcrime/seattle-parks-osm.geojson") %>%
  st_transform(crs = 2926)

Hospital <- 
  st_read("./dataWithoutcrime/Hospital.geojson") %>%
  st_transform(crs = 2926)

## Streetcar
streetcar <- 
  st_read("./dataWithoutcrime/streetcar_3857.geojson") %>%
  st_transform(crs = 2926)

## Traffic flow
traffic <- 
  st_read("./dataWithoutcrime/traffic_3857.geojson") %>%
  st_transform(crs = 2926)

## canopy
canopy <- 
  st_read("./dataWithoutcrime/tree_3857.geojson") %>%
  st_transform(crs = 2926)

## Crosswalk
crosswalk <- 
  st_read("./dataWithoutcrime/Marked_Crosswalks.geojson") %>%
  st_transform(crs = 2926)


```

## Current housing price map

```{r}
Seattle.sf_tracts2$quintiles <- ntile(Seattle.sf_tracts2$price_per_sqft, 5)

# Convert quintiles to a factor if not already done
Seattle.sf_tracts2$quintiles <- as.factor(Seattle.sf_tracts2$quintiles)

# Define the palette
Palette5 <- brewer.pal(5, "YlGnBu")

# Generate the plot
ggplot() +
  geom_sf(data = tracts, fill = "lightgrey", col = "white") +
  geom_sf(data = Seattle.sf_tracts2, aes(colour = quintiles), show.legend = "point", size = .25) +
  scale_colour_manual(values = Palette5,
                      labels = c("Low", "Relatively Low", "Medium", "Relatively High", "High"),
                      name = "Price Per Square Foot Quintiles") +
  labs(title = "Price Per Square Foot, Seattle") +
  theme_void()

```

## Clean Safety data
```{r}


Seattle.sf_tracts2$crime.Buffer <- Seattle.sf_tracts2 %>% 
    st_buffer(660) %>% 
    aggregate(dplyr::select(Crime.sf) %>% 
    mutate(counter = 1), ., sum)

## Nearest Neighbor Feature
Seattle.sf_tracts2 <-
  Seattle.sf_tracts2 %>% 
    mutate(
      crime_nn1 = nn_function(st_coordinates(Seattle.sf_tracts2), 
                              st_coordinates(Crime.sf), k = 1),
      
      crime_nn2 = nn_function(st_coordinates(Seattle.sf_tracts2), 
                              st_coordinates(Crime.sf), k = 2), 
      
      crime_nn3 = nn_function(st_coordinates(Seattle.sf_tracts2), 
                              st_coordinates(Crime.sf), k = 3), 
      
      crime_nn4 = nn_function(st_coordinates(Seattle.sf_tracts2), 
                              st_coordinates(Crime.sf), k = 4), 
      
      crime_nn5 = nn_function(st_coordinates(Seattle.sf_tracts2), 
                              st_coordinates(Crime.sf), k = 5)) 

```


## Schools

```{r}

#count the school number within the buffer
Seattle.sf_tracts2$school.Buffer <- Seattle.sf_tracts2 %>% 
    st_buffer(5280) %>% 
    aggregate(dplyr::select(School) %>% 
    mutate(counter = 1), ., sum)

## Nearest Neighbor Feature
Seattle.sf_tracts2 <-
  Seattle.sf_tracts2 %>% 
    mutate(
      school_nn1 = nn_function(st_coordinates(Seattle.sf_tracts2), 
                              st_coordinates(School), k = 1),
      
      school_nn2 = nn_function(st_coordinates(Seattle.sf_tracts2), 
                              st_coordinates(School), k = 2), 
      
      school_nn3 = nn_function(st_coordinates(Seattle.sf_tracts2), 
                              st_coordinates(School), k = 3), 
      
      school_nn4 = nn_function(st_coordinates(Seattle.sf_tracts2), 
                              st_coordinates(School), k = 4), 
      
      school_nn5 = nn_function(st_coordinates(Seattle.sf_tracts2), 
                              st_coordinates(School), k = 5))

```


## Hospitals

```{r}

Seattle.sf_tracts2$hosp.Buffer <- Seattle.sf_tracts2 %>% 
    st_buffer(5280) %>% 
    aggregate(dplyr::select(Hospital) %>% 
    mutate(counter = 1), ., sum)

## Nearest Neighbor Feature
Seattle.sf_tracts2 <-
  Seattle.sf_tracts2 %>% 
    mutate(
      hosp_nn1 = nn_function(st_coordinates(Seattle.sf_tracts2), 
                              st_coordinates(Hospital), k = 1))

```

## Parks

```{r}

#Parks are polygon hard to get distance
Seattle.sf_tracts2$parks.Buffer <- Seattle.sf_tracts2 %>% 
    st_buffer(1320) %>% 
    aggregate(dplyr::select(Parks) %>% 
    mutate(counter = 1), ., sum)

```

## crosswalk

```{r}

#Parks are polygon hard to get distance
Seattle.sf_tracts2$crosswalk.Buffer <- Seattle.sf_tracts2 %>% 
    st_buffer(1320) %>% 
    aggregate(dplyr::select(crosswalk) %>% 
    mutate(counter = 1), ., sum)

```


## Streetcar stations

```{r}

# Streetcar buffer
streetcar.Buffer <- st_union(st_buffer(streetcar, dist=1000)) 

# Generate the plot
ggplot() +
  geom_sf(data = tracts, fill = "lightgrey", col = "white") +
  geom_sf(data = streetcar.Buffer, fill = "orange", col = "white") +
  labs(title = "Streetcar buffer") +
  theme_void()

# Assign to overall dataframe
Seattle.sf_tracts2$streetcar.station <- st_within(Seattle.sf_tracts2, streetcar.Buffer) %>% lengths > 0

```

## canopy

```{r}

# join canopy to housing points
Seattle.sf_tracts3 <- st_join(Seattle.sf_tracts2, canopy, left = TRUE)

Seattle.sf_tracts3 <- Seattle.sf_tracts3 %>%
  st_drop_geometry() %>%
  select(id, Can_P)

# put the column into the overall dataframe
Seattle.sf_tracts2$canopy <- Seattle.sf_tracts3["Can_P"]

```

## distance to major roads

```{r}


# traffic_relation <- nearestsegment(as.ppp(Seattle.sf_tracts2), as.psp(traffic))


# Convert Seattle.sf_tracts2 to a point pattern, specifying mark column(s)
Seattle_points <- as.ppp(Seattle.sf_tracts2)

# Convert traffic to a line segment pattern
traffic_lines <- as.psp(traffic)

# Find nearest line segment to each point
nearest_segments <- nncross(Seattle_points, traffic_lines)

# put the column into the overall dataframe
Seattle.sf_tracts2$distance <- nearest_segments['dist']

# Assuming you have a dataframe 'df' and the column 'streetcar.station' is logical
Seattle.sf_tracts2$streetcar.station <- as.integer(Seattle.sf_tracts2$streetcar.station)


```
## Demographics

```{r}
Demo <- get_acs(geography = "tract", 
          variables = c("B01001_001E", "B01001A_001E", "B06011_001E"), 
          year = 2018, 
          state = "53", 
          county = "033", 
          geometry = T, 
          output = "wide") %>%
  st_transform(crs = 2926) %>%
  rename(TotalPop = B01001_001E,
         NumberWhites = B01001A_001E,
         MedIncome = B06011_001E) %>%
  mutate(pctWhite = NumberWhites / TotalPop)%>%
  select(-NAME, -GEOID)

Demo <- st_transform(Demo, st_crs(Seattle.sf_tracts2))

Seattle.sf_tracts2 <- st_join(Seattle.sf_tracts2, Demo, left = TRUE)
Seattle.sf_tracts2 <- Seattle.sf_tracts2 %>%
  select(-contains("B01"), -contains("B06"))


```

## Summarize and preview the variables
-stargazer

```{r preview the data}


```

# Analysis Variables

## Analyzing associations


```{r Correlation, fig.height=7, fig.width=10}

# Remove geometry column (if present), calculate Age, and select relevant variables
Seattle <- st_drop_geometry(Seattle.sf_tracts2) %>%
  mutate(Age = 2024 - yr_built, parksCount = parks.Buffer$counter, crimeCount=crime.Buffer$counter, schoolCount=school.Buffer$counter, hospitalCount=hosp.Buffer$counter, crosswalkCount=crosswalk.Buffer$counter, canopyPercent=canopy$Can_P, distance=distance$dist) %>%
  select(price, sqft_living, sqft_lot, Age, crimeCount, schoolCount, hospitalCount, parksCount, crosswalkCount, canopyPercent, distance, NumberWhites, MedIncome) %>%
  filter(Age < 500)
 

Seattle_long <- gather(Seattle, Variable, Value, -price)

# Create scatterplot with linear regression lines for each variable
ggplot(Seattle_long, aes(Value, price)) +   # Set up the plot
  geom_point(size = .5) +                       # Add points for each data point
  geom_smooth(method = "lm", se = FALSE, colour = "#FA7800") +  # Add linear regression lines
  facet_wrap(~Variable, ncol = 3, scales = "free") +  # Create separate plots for each variable
  labs(title = "Price as a function of continuous variables") +  # Set plot title
  theme_minimal()  # Apply a minimal theme to the plot

```


## Correlation matrix

A correlation matrix gives us the pairwise correlation of each set of features in our data. It is usually advisable to include the target/outcome variable in this so we can understand which features are related to it.

Some things to notice in this code; we use `select_if()` to select only the features that are numeric. This is really handy when you don't want to type or hard-code the names of your features; `ggcorrplot()` is a function from the `ggcorrplot` package.

**Let's take a few minutes to interpret this**

```{r fig.height=10, fig.width=10}
# Select only numeric variables and remove rows with missing values
numericVars <- Seattle.sf_tracts2 %>%
  st_drop_geometry() %>%  # Remove geometry column if present
  select_if(is.numeric) %>%  # Select only numeric variables
  na.omit()  # Remove rows with missing values

# Calculate correlation matrix
correlation_matrix <- cor(numericVars)


# yet another way to plot the correlation plot using the corrr library
numericVars %>% 
  correlate() %>% 
  autoplot() +
  geom_text(aes(label = round(r,digits=2)),size = 2)

```



## Multivariate Regression


```{r mutlivariate_regression}
Seattle_final_0 <- st_drop_geometry(Seattle.sf_tracts2) %>%
  mutate(Age = 2015 - yr_built, crime_count= crime.Buffer$counter, school_count=school.Buffer$counter, hosp_count=hosp.Buffer$counter, parks_count=parks.Buffer$counter, cross_count = crosswalk.Buffer$counter, canopy=canopy$Can_P, TrafficDis=distance$dist) %>%  
  select(-contains("Buffer"), -contains("distance"), -contains("date"), -yr_built)%>% 
  filter(price <= 2000000) 

reg0 <- lm(price ~ ., data = Seattle_final_0)

summary(reg0)

Seattle_final <- st_drop_geometry(Seattle.sf_tracts2) %>%
  mutate(Age = 2015 - yr_built, crime_count= crime.Buffer$counter, school_count=school.Buffer$counter, hosp_count=hosp.Buffer$counter, parks_count=parks.Buffer$counter, cross_count = crosswalk.Buffer$counter, canopy=canopy$Can_P, TrafficDis=distance$dist) %>%  
  select(id, Age, price, bedrooms, bathrooms, sqft_living, waterfront, view, condition, grade, crime_count, hosp_nn1, school_count, parks_count, cross_count, canopy, TrafficDis, pctWhite,  MedIncome) %>% 
  filter(price <= 2000000) 

Seattle_final <- Seattle_final %>%
  mutate_all(~replace(., is.na(.), 0))

reg1 <- lm(price ~ ., data = Seattle_final)

summary_reg1 <- summary(reg1)

coefficients_table <- as.data.frame(summary_reg1$coefficients)

coefficients_table$significance <- ifelse(coefficients_table$`Pr(>|t|)` < 0.001, '***',
                                    ifelse(coefficients_table$`Pr(>|t|)` < 0.01, '**',
                                      ifelse(coefficients_table$`Pr(>|t|)` < 0.05, '*',
                                        ifelse(coefficients_table$`Pr(>|t|)` < 0.1, '.', ''))))

coefficients_table$p_value <- paste0(round(coefficients_table$`Pr(>|t|)`, digits = 3), coefficients_table$significance)
coefficients_table$'t value' <- round(coefficients_table$'t value', digits = 2)
coefficients_table$'Std. Error' <- round(coefficients_table$'Std. Error', digits = 2)
coefficients_table$Estimate <- round(coefficients_table$Estimate, digits = 2)

coefficients_table %>%
  select(-significance, -`Pr(>|t|)`) %>% 
  kable(align = "r") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>%
  footnote(general_title = "\n", general = "Table x")


```

```{r}
model_summary <- data.frame(
  Statistic = c("Multiple R-squared", "Adjusted R-squared", "F-statistic"),
  Value = c(
    summary_reg1$r.squared,
    summary_reg1$adj.r.squared,
    summary_reg1$fstatistic[1]
  )
)

model_summary %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))%>%
  footnote(general_title = "\n", general = "Table x")
```

#Residual

```{r}
residuals_df <- data.frame(Residuals = resid(reg1), Fitted = fitted(reg1))
ggplot(residuals_df, aes(x = Fitted, y = Residuals)) +
  geom_point(size = 0.4, color = "black") +
  geom_hline(yintercept = 0, color = "red") +
  labs(title = "Residual Plot for Regression",
       subtitle = "Each dot represent one property ",
       x = "Fitted Values",
       y = "Residuals") +
  theme_light() +   
  theme(plot.subtitle = element_text(size = 9,face = "italic"),
        plot.title = element_text(size = 12, face = "bold"), 
        axis.text.x=element_text(size=6),
        axis.text.y=element_text(size=6), 
        axis.title=element_text(size=8))
```

#Training and Testing Datasets

```{r traintest}

# Split the dataset into a training set and a test set using stratified sampling
inTrain <- createDataPartition(
              y = paste(Seattle_final$sqft_lot, Seattle_final$view, 
                        Seattle_final$yr_built, Seattle_final$school_nn1), 
              p = .60, list = FALSE)  # Create a vector of indices for the training set

# Subset the dataset to create the training set
Seattle.training <- Seattle_final[inTrain,]  # Training set
# Subset the dataset to create the test set
Seattle.test <- Seattle_final[-inTrain,]     # Test set
 
# Fit a linear regression model to predict Sales Price using selected predictors
reg.training <- 
  lm(price ~ ., data = as.data.frame(Seattle.training))

summary_training <- summary(reg.training)

# Make predictions on the test set and evaluate model performance

Seattle.test <-
  Seattle.test %>%  # Pipe the test set into the following operations
  # Add a column indicating the type of regression model used
  mutate(Regression = "Baseline Regression",
         # Predict sale prices using the trained regression model
         SalePrice.Predict = predict(reg.training, Seattle.test),
         # Calculate the difference between predicted and actual sale prices
         SalePrice.Error = SalePrice.Predict - price,
         # Calculate the absolute difference between predicted and actual sale prices
         SalePrice.AbsError = abs(SalePrice.Predict - price),
         # Calculate the absolute percentage error
         SalePrice.APE = (abs(SalePrice.Predict - price)) / price) %>%
  filter(price < 5000000)  # Filter out records with SalePrice greater than $5,000,000


```

## examine the model (diagnose)

```{r}

#MAE
mean(Seattle.test$SalePrice.AbsError, na.rm = T)

## absolute difference between the predicted value (often a forecast or an estimate produced by a model) and the actual value observed.
## larger value means larger error, less accurate of the model

#MAPE
mean(Seattle.test$SalePrice.APE, na.rm = T)

## Mean Absolute Percentage Error
## the error as a percentage of the actual values

## Plot of marginal response
effect_plot(reg1, pred = crime_count, interval = TRUE, plot.points = TRUE)

## Plot coefficients
plot_summs(reg1, scale = TRUE)

## plot multiple model coeffs, use another model for comparison.
# plot_summs(reg1, reg2)

```

# cross validation
This is a slightly nicer way of doing the CV as compared to the warm-up example.
```{r cv}
# Load necessary libraries

# Define the control parameters for k-fold cross-validation
control <- trainControl(method = "cv",    # Use k-fold cross-validation
                        number = 10,      # Number of folds
                        verboseIter = TRUE,  # Show verbose output
                        returnData = FALSE,  # Don't return resampled data
                        savePredictions = TRUE,  # Save predictions
                        classProbs = FALSE,  # Don't compute class probabilities
                        summaryFunction = defaultSummary)  # Use default summary function

# Train the linear regression model using k-fold cross-validation
lm_cv <- train(price ~ .,  # Formula for the linear regression model
               data = Seattle_final,  # Dataset
               method = "lm",           # Specify "lm" for linear regression
               trControl = control)     # Use the defined control parameters

# View the cross-validation results
print(lm_cv)


# Plot observed versus predicted values
plot(lm_cv$pred$obs, lm_cv$pred$pred, 
     xlab = "Observed", ylab = "Predicted", 
     main = "Observed vs Predicted Values") +
     abline(0, 1, col = "red")


```

# Distribution of Sale Price Error Testing Set

```{r}



Seattle.test_join <- left_join(Seattle.test, housingData, join_by(id == id))
Seattle.test_withgeo <- 
  Seattle.test_join %>% 
  st_as_sf(coords = c("long", "lat"), crs = 4326, agr = "constant") %>%
  st_transform(crs = 2926)

Seattle.test_withgeo$SalePrice.Error <- round(Seattle.test_withgeo$SalePrice.Error, digits = 2)

ggplot()+
  geom_sf(data = tracts, fill = "lightgrey", col = "white") +
  geom_sf(data=Seattle.test_withgeo, aes(colour = Seattle.test_withgeo$SalePrice.Error), size=0.5)+
  scale_color_continuous(high = "blue", low = "red", name= "Sale Price Error ") +
  labs(title = "Distribution of Sale Price Error Testing Set") +
  theme_void()
```


#Mean Absolute Percentage Error By Neighborhood

```{r}
to_plot <- st_intersection(Seattle.test_withgeo, neighborhood %>% dplyr::select("S_HOOD")) %>% 
  st_drop_geometry() %>% 
  group_by(S_HOOD) %>%
  summarise(mean.MAPE = mean(SalePrice.APE, na.rm = T)) %>% 
  left_join(neighborhood) %>% 
  st_sf()
to_plot %>%
  ggplot() + 
      geom_sf(aes(fill = mean.MAPE)) +
  scale_fill_continuous(low = "blue", high = "red", name= "MAPE") +
  theme(
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        plot.title = element_text(size = 12, face = "bold"),
        panel.background = element_blank(),
        panel.border = element_rect(colour = "grey", fill=NA, size=0.4)
        ) +
  labs(title = "Mean Absolute Percentage Error By Neighborhood") 
```


#RMSE, MAE


```{r loocv}
# Set up leave-one-out cross-validation
control <- trainControl(method = "LOOCV",     # Use leave-one-out cross-validation
                        number = nrow(Seattle_final),  # Number of folds = number of observations
                        verboseIter = TRUE,  # Show verbose output
                        returnData = FALSE,  # Don't return resampled data
                        savePredictions = TRUE,  # Save predictions
                        classProbs = FALSE,  # Don't compute class probabilities
                        summaryFunction = defaultSummary)  # Use default summary function

# Train the linear regression model using leave-one-out cross-validation
lm_loocv <- train(price ~ .,  # Formula for the linear regression model
                  data = Seattle_final,  # Dataset
                  method = "lm",           # Specify "lm" for linear regression
                  trControl = control)     # Use the defined control parameters

# View the cross-validation results
# print(lm_loocv)
# 6669 fold in total

```

```{r}

statistic_summary <- data.frame(
  "RMSE" = c("133631.9"),
  "RSquared" = c("0.7644132"),
  "MAE" = c("94924.28"))
  
kbl(statistic_summary) %>%
  kable_styling(bootstrap_options = "striped", full_width = T, position = "left")

```


The Rsquared value indicates that around XX.XX% of the variability in SalePrice is explained by the model's predictor variables.



