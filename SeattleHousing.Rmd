---
title: "Predict Housing Value"
author: "Ziyi GUO, Junyi Yang"
output: html_document
---


```{r setup, include=FALSE}

# You can set some global options for knitting chunks

knitr::opts_chunk$set(echo = TRUE)

# Load some libraries

library(tidyverse)
library(sf)
library(spdep)
library(caret)
library(ckanr)
library(FNN)
library(grid)
library(gridExtra)
library(ggcorrplot) # plot correlation plot
library(corrr)      # another way to plot correlation plot
library(kableExtra)
library(jtools)     # for regression model plots
library(ggstance) # to support jtools plots
library(ggpubr)    # plotting R^2 value on ggplot point scatter
library(broom.mixed) # needed for effects plots
library(RColorBrewer)
library(stargazer)
library(dplyr)
library(ggplot2)
library(RColorBrewer)
# library(geosphere)
# library(parallel)
# library(sp)
library(spatstat)

# functions and data directory
root.dir = "https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/"

source("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r")

```


## Data Wrangling

```{r get data}

# initial housing data
housingData <- 
  read.csv(file.path("./dataWithoutcrime/kc_house_data.csv"))

Seattle.sf <- 
  housingData %>% 
  st_as_sf(coords = c("long", "lat"), crs = 4326, agr = "constant") %>%
  st_transform(crs = 2926)%>%
  mutate(price_per_sqft = price / sqft_lot)

# Seattle geo_Tracts
tracts <- 
  st_read("./dataWithoutcrime/2010_Census_Tract_Seattle.geojson") %>%
  st_transform(crs = 2926)

#Cut the housing data to fit into the Seattle boundary
Seattle.sf_tracts <- st_intersection(Seattle.sf, tracts)

Seattle.sf_tracts2 <- Seattle.sf_tracts %>%
  select(id, date, price ,bedrooms, bathrooms, sqft_living, sqft_lot, floors, waterfront, view, condition, grade, yr_built, yr_renovated, zipcode, price_per_sqft, geometry)


# Add-in Data
## social
#crime <- 
#  read.csv(file.path("C:/Users/25077/Desktop/PPA_Midterm/data/SPD_Crime_Data.csv"))

crime <- 
  read.csv(file.path("SPD_Crime_Data__2008-Present_20240319.csv"))

Crime.sf <- 
  crime %>% 
  st_as_sf(coords = c("Longitude", "Latitude"), crs = 4326, agr = "constant") %>%
  st_transform(crs = 2926)


## Amenities
School <- 
  st_read("./dataWithoutcrime/Private_School.geojson") %>%
  st_transform(crs = 2926)

Parks <- 
  st_read("./dataWithoutcrime/seattle-parks-osm.geojson") %>%
  st_transform(crs = 2926)

Hospital <- 
  st_read("./dataWithoutcrime/Hospital.geojson") %>%
  st_transform(crs = 2926)

## Streetcar
streetcar <- 
  st_read("./dataWithoutcrime/streetcar_3857.geojson") %>%
  st_transform(crs = 2926)

## Traffic flow
traffic <- 
  st_read("./dataWithoutcrime/traffic_3857.geojson") %>%
  st_transform(crs = 2926)

## canopy
canopy <- 
  st_read("./dataWithoutcrime/tree_3857.geojson") %>%
  st_transform(crs = 2926)

## Transportation

# street car

# Crosswalk
crosswalk <- 
  st_read("C:/Users/25077/Desktop/MUSA 508_PPA/5_/github/local/seattle-housing/dataWithoutcrime/Marked_Crosswalks.geojson") %>%
  st_transform(crs = 2926)


```

## Summarize and preview the variables
-stargazer

```{r preview the data}


```
## Current housing price map

```{r}
Seattle.sf_tracts2$quintiles <- ntile(Seattle.sf_tracts2$price_per_sqft, 5)

# Convert quintiles to a factor if not already done
Seattle.sf_tracts2$quintiles <- as.factor(Seattle.sf_tracts2$quintiles)

# Define the palette
Palette5 <- brewer.pal(5, "YlGnBu")

# Generate the plot
ggplot() +
  geom_sf(data = tracts, fill = "lightgrey", col = "white") +
  geom_sf(data = Seattle.sf_tracts2, aes(colour = quintiles), show.legend = "point", size = .25) +
  scale_colour_manual(values = Palette5,
                      labels = c("Low", "Relatively Low", "Medium", "Relatively High", "High"),
                      name = "Price Per Square Foot Quintiles") +
  labs(title = "Price Per Square Foot, Seattle") +
  theme_void()

```

## Clean Safety data
```{r}

#review crime types
crime %>% 
group_by(Offense.Parent.Group) %>%
  summarize(count = n()) %>%
  arrange(-count) %>% top_n(10) %>%
  kable() %>%
  kable_styling()

Seattle.sf_tracts2$crime.Buffer <- Seattle.sf_tracts2 %>% 
    st_buffer(660) %>% 
    aggregate(dplyr::select(Crime.sf) %>% 
    mutate(counter = 1), ., sum)

## Nearest Neighbor Feature
Seattle.sf_tracts2 <-
  Seattle.sf_tracts2 %>% 
    mutate(
      crime_nn1 = nn_function(st_coordinates(Seattle.sf_tracts2), 
                              st_coordinates(Crime.sf), k = 1),
      
      crime_nn2 = nn_function(st_coordinates(Seattle.sf_tracts2), 
                              st_coordinates(Crime.sf), k = 2), 
      
      crime_nn3 = nn_function(st_coordinates(Seattle.sf_tracts2), 
                              st_coordinates(Crime.sf), k = 3), 
      
      crime_nn4 = nn_function(st_coordinates(Seattle.sf_tracts2), 
                              st_coordinates(Crime.sf), k = 4), 
      
      crime_nn5 = nn_function(st_coordinates(Seattle.sf_tracts2), 
                              st_coordinates(Crime.sf), k = 5)) 

```


## Schools

```{r}

#count the school number within the buffer
Seattle.sf_tracts2$school.Buffer <- Seattle.sf_tracts2 %>% 
    st_buffer(5280) %>% 
    aggregate(dplyr::select(School) %>% 
    mutate(counter = 1), ., sum)

## Nearest Neighbor Feature
Seattle.sf_tracts2 <-
  Seattle.sf_tracts2 %>% 
    mutate(
      school_nn1 = nn_function(st_coordinates(Seattle.sf_tracts2), 
                              st_coordinates(School), k = 1),
      
      school_nn2 = nn_function(st_coordinates(Seattle.sf_tracts2), 
                              st_coordinates(School), k = 2), 
      
      school_nn3 = nn_function(st_coordinates(Seattle.sf_tracts2), 
                              st_coordinates(School), k = 3), 
      
      school_nn4 = nn_function(st_coordinates(Seattle.sf_tracts2), 
                              st_coordinates(School), k = 4), 
      
      school_nn5 = nn_function(st_coordinates(Seattle.sf_tracts2), 
                              st_coordinates(School), k = 5))

```


## Hospitals

```{r}

Seattle.sf_tracts2$hosp.Buffer <- Seattle.sf_tracts2 %>% 
    st_buffer(5280) %>% 
    aggregate(dplyr::select(Hospital) %>% 
    mutate(counter = 1), ., sum)

## Nearest Neighbor Feature
Seattle.sf_tracts2 <-
  Seattle.sf_tracts2 %>% 
    mutate(
      hosp_nn1 = nn_function(st_coordinates(Seattle.sf_tracts2), 
                              st_coordinates(Hospital), k = 1),
      
      hosp_nn2 = nn_function(st_coordinates(Seattle.sf_tracts2), 
                              st_coordinates(Hospital), k = 2), 
      
      hosp_nn3 = nn_function(st_coordinates(Seattle.sf_tracts2), 
                              st_coordinates(Hospital), k = 3), 
      
      hosp_nn4 = nn_function(st_coordinates(Seattle.sf_tracts2), 
                              st_coordinates(Hospital), k = 4), 
      
      hosp_nn5 = nn_function(st_coordinates(Seattle.sf_tracts2), 
                              st_coordinates(Hospital), k = 5))

```

## Parks

```{r}

#Parks are polygon hard to get distance
Seattle.sf_tracts2$parks.Buffer <- Seattle.sf_tracts2 %>% 
    st_buffer(1320) %>% 
    aggregate(dplyr::select(Parks) %>% 
    mutate(counter = 1), ., sum)

```

## crosswalk

```{r}

#Parks are polygon hard to get distance
Seattle.sf_tracts2$crosswalk.Buffer <- Seattle.sf_tracts2 %>% 
    st_buffer(1320) %>% 
    aggregate(dplyr::select(crosswalk) %>% 
    mutate(counter = 1), ., sum)

```


## Streetcar stations

```{r}

# Streetcar buffer
streetcar.Buffer <- st_union(st_buffer(streetcar, dist=1000)) 

# Generate the plot
ggplot() +
  geom_sf(data = tracts, fill = "lightgrey", col = "white") +
  geom_sf(data = streetcar.Buffer, fill = "orange", col = "white") +
  labs(title = "Streetcar buffer") +
  theme_void()

# Assign to overall dataframe
Seattle.sf_tracts2$streetcar.station <- st_within(Seattle.sf_tracts2, streetcar.Buffer) %>% lengths > 0

```

## canopy

```{r}

# join canopy to housing points
Seattle.sf_tracts3 <- st_join(Seattle.sf_tracts2, canopy, left = TRUE)

Seattle.sf_tracts3 <- Seattle.sf_tracts3 %>%
  st_drop_geometry() %>%
  select(id, Can_P)

# put the column into the overall dataframe
Seattle.sf_tracts2$canopy <- Seattle.sf_tracts3["Can_P"]

```

## distance to major roads

```{r}


# traffic_relation <- nearestsegment(as.ppp(Seattle.sf_tracts2), as.psp(traffic))


# Convert Seattle.sf_tracts2 to a point pattern, specifying mark column(s)
Seattle_points <- as.ppp(Seattle.sf_tracts2)

# Convert traffic to a line segment pattern
traffic_lines <- as.psp(traffic)

# Find nearest line segment to each point
nearest_segments <- nncross(Seattle_points, traffic_lines)

# put the column into the overall dataframe
Seattle.sf_tracts2$distance <- nearest_segments['dist']

```


```{r}

# need to use 4326 for the following analysis
# traffic <- st_transform(traffic, crs = 4326)
# Seattle.sf_tracts2 <- st_transform(Seattle.sf_tracts2, crs = 4326)
# 
# # converts the extracted geometry vector into a Spatial object
# points.sp <- as_Spatial(st_geometry(Seattle.sf_tracts2))
# # add index
# points.sp$system.index <- Seattle.sf_tracts2$id
# # do the same for line
# lines.sp <- as_Spatial(st_geometry(traffic), IDs = as.character(traffic$OBJECTID)) 
# 
# 
# # Calculate the number of cores
# no_cores <- detectCores() - 1
# 
# # Split features in n parts
# n <- 100 # split pixels by 100 points
# parts <- split(1:nrow(Seattle.sf_tracts2), cut(1:nrow(Seattle.sf_tracts2), n))
# 
# lines.sp <- as_Spatial(st_geometry(traffic), IDs = as.character(traffic$OBJECTID)) 
# 
# # Initiate cluster (after loading all the necessary object to R environment: BRA_adm2, parts, r.raster, n)
# cl <- makeCluster(no_cores, type = "PSOCK")
# print(cl)
# 
# # Multithread dist2Line function ------------------------------------------
# 
# # Parallelize dist2Line function
# system.time(distParts <- parLapply(cl = cl, 
#                                    X = 1:n, 
#                                    fun = function(x) {
#                                      points.sp <- as_Spatial(st_geometry(Seattle.sf_tracts2[parts[[x]],]))
#                                      points.sp$system.index <- Seattle.sf_tracts2$id[parts[[x]]]
#                                      dist <- geosphere::dist2Line(p = points.sp, line = lines.sp)
#                                      # Convert dist to data.frame
#                                      dist.df <- as.data.frame(dist)
#                                      # Add points ID column to dist.df
#                                      dist.df$system.index <- Seattle.sf_tracts2$id[parts[[x]]]
#                                      colnames(dist.df) <- c("distance", "lon", "lat", "OBJECTID", "system.index")
#                                      gc(verbose = FALSE) # free memory
#                                      return(dist.df)
#                                    }))
# 
# # user   system  elapsed 
# # 1.869    2.067 1314.271
# 
# # Finish
# stopCluster(cl)
# 
# # Bind rows
# distBind <- do.call("rbind", distParts)
# 
# 
# 
# # Calculate the number of cores
# no_cores <- detectCores() - 1
# 
# # Split features in n parts
# n <- 100 # split pixels by 100 points
# parts <- split(1:nrow(Seattle.sf_tracts2), cut(1:nrow(Seattle.sf_tracts2), n))
# 
# lines.sp <- sf::st_as_sf(traffic, IDs = as.character(traffic$OBJECTID))
# 
# # Initiate cluster
# cl <- makeCluster(no_cores, type = "PSOCK")
# print(cl)
# 
# clusterExport(cl, c("Seattle.sf_tracts2", "traffic"))
# clusterEvalQ(cl, library(sf))
# clusterEvalQ(cl, library(sp))
# clusterEvalQ(cl, library(geosphere))
# 
# # Multithread dist2Line function ------------------------------------------
# 
# # Parallelize dist2Line function
# system.time(distParts <- parLapply(cl, parts, function(x) {
#   points.sp <- sf::st_as_sf(Seattle.sf_tracts2[parts[[x]], ])
#   points.sp$system.index <- Seattle.sf_tracts2$id[parts[[x]]]
#   dist <- geosphere::dist2Line(p = points.sp, line = lines.sp)
#   dist.df <- as.data.frame(dist)
#   dist.df$system.index <- Seattle.sf_tracts2$id[parts[[x]]]
#   colnames(dist.df) <- c("distance", "lon", "lat", "OBJECTID", "system.index")
#   return(dist.df)
# }))
# 
# # Finish
# stopCluster(cl)
# 
# # Bind rows
# distBind <- do.call("rbind", distParts)

```


# Analysis Variables

## Analyzing associations


```{r Correlation}

# Remove geometry column (if present), calculate Age, and select relevant variables
Seattle <- st_drop_geometry(Seattle.sf_tracts2) %>%
  mutate(Age = 2024 - yr_built, parksCount = parks.Buffer$counter, crimeCount=crime.Buffer$counter) %>%
  select(price, sqft_lot, Age, crime_nn1, parksCount, waterfront, crimeCount) %>%
  filter(Age < 500)
 

Seattle_long <- gather(Seattle, Variable, Value, -price)

# Create scatterplot with linear regression lines for each variable
ggplot(Seattle_long, aes(Value, price)) +   # Set up the plot
  geom_point(size = .5) +                       # Add points for each data point
  geom_smooth(method = "lm", se = FALSE, colour = "#FA7800") +  # Add linear regression lines
  facet_wrap(~Variable, ncol = 3, scales = "free") +  # Create separate plots for each variable
  labs(title = "Price as a function of continuous variables") +  # Set plot title
  theme_minimal()  # Apply a minimal theme to the plot

```

Next, we will look at correlations for the crime variables (kNN)

```{r crime_corr}

# Clean and prepare the data
Seattle_Crime <- Seattle.sf_tracts2 %>%
  st_drop_geometry() %>%                   # Remove geometry column (if present)
  mutate(Age = 2015 - yr_built, crime_count= crime.Buffer$counter) %>%        # Calculate Age based on the year built
  select(price, starts_with("crime_")) %>%  # Select variables related to crime
  filter(price <= 1000000)             # Filter data based on SalePrice

# Reshape data from wide to long format
Seattle_Crime_long <- gather(Seattle_Crime, Variable, Value, -price)

# Create scatterplot with linear regression lines for each variable
ggplot(Seattle_Crime_long, aes(Value, price)) +   # Set up the plot
  geom_point(size = .5) +                       # Add points for each data point
  geom_smooth(method = "lm", se = FALSE, colour = "#FA7800") +  # Add linear regression lines
  facet_wrap(~Variable, nrow = 1, scales = "free") +  # Create separate plots for each variable
  labs(title = "Price as a function of continuous variables") +  # Set plot title
  theme_minimal()  # Apply a minimal theme to the plot

```

## Correlation matrix

A correlation matrix gives us the pairwise correlation of each set of features in our data. It is usually advisable to include the target/outcome variable in this so we can understand which features are related to it.

Some things to notice in this code; we use `select_if()` to select only the features that are numeric. This is really handy when you don't want to type or hard-code the names of your features; `ggcorrplot()` is a function from the `ggcorrplot` package.

**Let's take a few minutes to interpret this**

```{r correlation_matrix}
# Select only numeric variables and remove rows with missing values
numericVars <- Seattle.sf_tracts2 %>%
  st_drop_geometry() %>%  # Remove geometry column if present
  select_if(is.numeric) %>%  # Select only numeric variables
  na.omit()  # Remove rows with missing values

# Calculate correlation matrix
correlation_matrix <- cor(numericVars)

# Create correlation plot
ggcorrplot(
  correlation_matrix,  # Correlation matrix
  p.mat = cor_pmat(numericVars),  # p-values for significance
  colors = c("#25CB10", "white", "#FA7800"),  # Custom color palette
  type = "lower",  # Lower triangle of the correlation matrix
  insig = "blank"  # Hide insignificant correlations
) +
labs(title = "Correlation across numeric variables")  # Set plot title


# yet another way to plot the correlation plot using the corrr library
numericVars %>% 
  correlate() %>% 
  autoplot() +
  geom_text(aes(label = round(r,digits=2)),size = 2)

```



## Multivariate Regression


```{r mutlivariate_regression}

Seattle_final <- st_drop_geometry(Seattle.sf_tracts2) %>%
  mutate(Age = 2015 - yr_built, crime_count= crime.Buffer$counter, school_count=school.Buffer$counter, hosp_count=hosp.Buffer$counter, parks_count=parks.Buffer$counter, cross_count = crosswalk.Buffer$counter) %>%  
  select(Age, price, bedrooms, bathrooms, sqft_living, floors, waterfront, view, condition, grade, crime_count, hosp_nn1, school_nn1, parks_count, cross_count) %>% 
  filter(price <= 2000000) 

Seattle_final <- Seattle_final %>%
  mutate_all(~replace(., is.na(.), 0))

reg1 <- lm(price ~ ., data = Seattle_final)

## comparison model
reg2 <- lm(price ~ cross_count, data = Seattle_final)

summary(reg1)
```


#Training and Testing Datasets

```{r traintest}

# Split the dataset into a training set and a test set using stratified sampling
inTrain <- createDataPartition(
              y = paste(Seattle_final$sqft_lot, Seattle_final$view, 
                        Seattle_final$yr_built, Seattle_final$school_nn1), 
              p = .60, list = FALSE)  # Create a vector of indices for the training set

# Subset the dataset to create the training set
Seattle.training <- Seattle_final[inTrain,]  # Training set
# Subset the dataset to create the test set
Seattle.test <- Seattle_final[-inTrain,]     # Test set
 
# Fit a linear regression model to predict Sales Price using selected predictors
reg.training <- 
  lm(price ~ ., data = as.data.frame(Seattle.training))

summary(reg.training)

# Make predictions on the test set and evaluate model performance

Seattle.test <-
  Seattle.test %>%  # Pipe the test set into the following operations
  # Add a column indicating the type of regression model used
  mutate(Regression = "Baseline Regression",
         # Predict sale prices using the trained regression model
         SalePrice.Predict = predict(reg.training, Seattle.test),
         # Calculate the difference between predicted and actual sale prices
         SalePrice.Error = SalePrice.Predict - price,
         # Calculate the absolute difference between predicted and actual sale prices
         SalePrice.AbsError = abs(SalePrice.Predict - price),
         # Calculate the absolute percentage error
         SalePrice.APE = (abs(SalePrice.Predict - price)) / price)
```

## examine the model (diagnose)

```{r}

#MAE
mean(Seattle.test$SalePrice.AbsError, na.rm = T)

## absolute difference between the predicted value (often a forecast or an estimate produced by a model) and the actual value observed.
## larger value means larger error, less accurate of the model

#MAPE
mean(Seattle.test$SalePrice.APE, na.rm = T)

## Mean Absolute Percentage Error
## the error as a percentage of the actual values

## Plot of marginal response
effect_plot(reg1, pred = crime_count, interval = TRUE, plot.points = TRUE)

## Plot coefficients
plot_summs(reg1, scale = TRUE)

## plot multiple model coeffs, use another model for comparison.
plot_summs(reg1, reg2)

```

# cross validation
This is a slightly nicer way of doing the CV as compared to the warm-up example.
```{r cv}
# Load necessary libraries

# Define the control parameters for k-fold cross-validation
control <- trainControl(method = "cv",    # Use k-fold cross-validation
                        number = 10,      # Number of folds
                        verboseIter = TRUE,  # Show verbose output
                        returnData = FALSE,  # Don't return resampled data
                        savePredictions = TRUE,  # Save predictions
                        classProbs = FALSE,  # Don't compute class probabilities
                        summaryFunction = defaultSummary)  # Use default summary function

# Train the linear regression model using k-fold cross-validation
lm_cv <- train(price ~ .,  # Formula for the linear regression model
               data = Seattle_final,  # Dataset
               method = "lm",           # Specify "lm" for linear regression
               trControl = control)     # Use the defined control parameters

# View the cross-validation results
print(lm_cv)


# Plot observed versus predicted values
plot(lm_cv$pred$obs, lm_cv$pred$pred, 
     xlab = "Observed", ylab = "Predicted", 
     main = "Observed vs Predicted Values") +
     abline(0, 1, col = "red")


```



RMSE, MAE


```{r loocv}
# Set up leave-one-out cross-validation
control <- trainControl(method = "LOOCV",     # Use leave-one-out cross-validation
                        number = nrow(Seattle_final),  # Number of folds = number of observations
                        verboseIter = TRUE,  # Show verbose output
                        returnData = FALSE,  # Don't return resampled data
                        savePredictions = TRUE,  # Save predictions
                        classProbs = FALSE,  # Don't compute class probabilities
                        summaryFunction = defaultSummary)  # Use default summary function

# Train the linear regression model using leave-one-out cross-validation
lm_loocv <- train(price ~ .,  # Formula for the linear regression model
                  data = Seattle_final,  # Dataset
                  method = "lm",           # Specify "lm" for linear regression
                  trControl = control)     # Use the defined control parameters

# View the cross-validation results
print(lm_loocv)

```

The Rsquared value indicates that around XX.XX% of the variability in SalePrice is explained by the model's predictor variables.



## Try to engineer a 'dummy variable' out of the other variables in an attempt to parameterize a variable that suggests a big or fancy house or levels of fanciness.
## How does this affect your model?

```{r}


```